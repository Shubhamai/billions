This is Part 1 in 3 Part series about training a 1 Billion Parameter model on 40 Billion tokens. The model was trained on a section of the slimpajama database on 8xH100 for 40 hours. The model was then later finetuning for chat is hosted on Replicate.

The model architecture is very similar to the recent Llama 3.2 1B model,

### Positional Embeddings

## Resources

- [L](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)
- [text](https://www.manning.com/books/build-a-large-language-model-from-scratch?utm_source=raschka&utm_medium=affiliate&utm_campaign=book_raschka_build_12_12_23&a_aid=raschka&a_bid=4c2437a0&chan=mm_github)
